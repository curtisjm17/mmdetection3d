{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10be1fec",
   "metadata": {},
   "source": [
    "## Load Required Libraries to Support Code Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca7a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Libraries\n",
    "from argparse import ArgumentParser\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from mmdet3d.utils import get_root_logger\n",
    "from mmdet3d.apis import inference_detector, inference_multi_modality_detector, init_model, show_result_meshlab\n",
    "from mmdet3d.datasets import build_dataloader, build_dataset\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from os import path as osp\n",
    "from mmdet3d.core import (Box3DMode, CameraInstance3DBoxes, Coord3DMode,\n",
    "                          DepthInstance3DBoxes, LiDARInstance3DBoxes,\n",
    "                          show_multi_modality_result, show_result,\n",
    "                          show_seg_result, show_results_modified)\n",
    "from mmdet3d.core import show_results_modified as show_results_modified\n",
    "from mmdet3d.core.evaluation.kitti_utils import kitti_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00b2f7",
   "metadata": {},
   "source": [
    "## Import and Initialize SECOND 3D Object Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1078662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmiller/Documents/Curtis_Classes/CMPE_249/Project/code/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:84: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ./checkpoints/hv_second_secfpn_6x8_80e_kitti-3d-3class_20210831_022017-ae782e87.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMDataParallel(\n",
       "  (module): VoxelNet(\n",
       "    (backbone): SECOND(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (8): ReLU(inplace=True)\n",
       "          (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (11): ReLU(inplace=True)\n",
       "          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (14): ReLU(inplace=True)\n",
       "          (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (17): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}\n",
       "    (neck): SECONDFPN(\n",
       "      (deblocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
       "    (bbox_head): Anchor3DHead(\n",
       "      (loss_cls): FocalLoss()\n",
       "      (loss_bbox): SmoothL1Loss()\n",
       "      (loss_dir): CrossEntropyLoss(avg_non_ignore=False)\n",
       "      (conv_cls): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_reg): Conv2d(512, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_dir_cls): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'conv_cls', 'std': 0.01, 'bias_prob': 0.01}}\n",
       "    (voxel_layer): Voxelization(voxel_size=[0.05, 0.05, 0.1], point_cloud_range=[0, -40, -3, 70.4, 40, 1], max_num_points=5, max_voxels=(16000, 40000), deterministic=True)\n",
       "    (voxel_encoder): HardSimpleVFE()\n",
       "    (middle_encoder): SparseEncoder(\n",
       "      (conv_input): SparseSequential(\n",
       "        (0): SubMConv3d(4, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (encoder_layers): SparseSequential(\n",
       "        (encoder_layer1): SparseSequential(\n",
       "          (0): SparseSequential(\n",
       "            (0): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer2): SparseSequential(\n",
       "          (0): SparseSequential(\n",
       "            (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): SparseSequential(\n",
       "            (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SparseSequential(\n",
       "            (0): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer3): SparseSequential(\n",
       "          (0): SparseSequential(\n",
       "            (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): SparseSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SparseSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer4): SparseSequential(\n",
       "          (0): SparseSequential(\n",
       "            (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): SparseSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SparseSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_out): SparseSequential(\n",
       "        (0): SparseConv3d(64, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)\n",
       "        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model from a config file and a checkpoint file\n",
    "# Specify model to be used\n",
    "config_filename = './configs/second/hv_second_secfpn_6x8_80e_kitti-3d-3class.py'\n",
    "checkpoint_filename = './checkpoints/hv_second_secfpn_6x8_80e_kitti-3d-3class_20210831_022017-ae782e87.pth'\n",
    "\n",
    "# Initialize Model\n",
    "model_single = init_model(config_filename, checkpoint_filename, device='cuda:0')\n",
    "\n",
    "# Get dataset structure that contains truth information\n",
    "cfg = model_single.cfg\n",
    "model = MMDataParallel(model_single)\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "\n",
    "# Setup dataloader structure that allows for easy to data for each data capture\n",
    "test_dataloader_default_args = dict(samples_per_gpu=1, workers_per_gpu=2, dist=False, shuffle=False)\n",
    "test_loader_cfg = {**test_dataloader_default_args, **cfg.data.get('test_dataloader', {})}\n",
    "data_loader = build_dataloader(dataset, **test_loader_cfg)\n",
    "\n",
    "# Startup model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cd585",
   "metadata": {},
   "source": [
    "## Function to Convert Data to Plottable Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107abe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_2_plot_frame(input_data, points, pred_boxes, gt_annos):\n",
    "    \n",
    "    # Convert LiDAR points to a plotable frame\n",
    "    points   = Coord3DMode.convert(points, box_mode, Coord3DMode.DEPTH)\n",
    "    \n",
    "    # Convert predicted bounding boxes to a plotable frame\n",
    "    show_pred_bboxes = Box3DMode.convert(pred_bboxes, box_mode, Box3DMode.DEPTH)\n",
    "\n",
    "    # Extract 3D boxes from ground truth data\n",
    "    gt_bbox = np.zeros((len(gt_annos['score']),7))\n",
    "    for i in range(0, len(gt_annos['score'])):\n",
    "        gt_bbox[i] = np.concatenate((gt_annos['location'][i], gt_annos['dimensions'][i],[gt_annos['rotation_y'][i]]))\n",
    "    \n",
    "    # Convert ground truth 3D boxes to a plotable frame\n",
    "    rect = input_data['calib']['R0_rect'].astype(np.float32)\n",
    "    Trv2c = input_data['calib']['Tr_velo_to_cam'].astype(np.float32)\n",
    "    gt_bboxes_3d = CameraInstance3DBoxes(gt_bbox).convert_to(Box3DMode.LIDAR, np.linalg.inv(rect @ Trv2c))\n",
    "    show_gt_bboxes = Box3DMode.convert(gt_bboxes_3d.tensor.numpy(), Box3DMode.LIDAR, Box3DMode.DEPTH)\n",
    "\n",
    "    return points, show_pred_bboxes, show_gt_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426fe11c",
   "metadata": {},
   "source": [
    "## Evaluate and Display Single Test LiDAR Scan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42563fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmiller/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Get LiDAR Data\n",
    "input_data = dataset.data_infos[0]\n",
    "pcd_file = './data/kitti/{}'.format(input_data['point_cloud']['velodyne_path'])\n",
    "\n",
    "# Run Point Cloud through Model\n",
    "result, data = inference_detector(model_single, pcd_file)\n",
    "\n",
    "# Get bounding boxes and scores of model predictions\n",
    "pred_bboxes = result[0]['boxes_3d'].tensor.numpy()\n",
    "pred_scores = result[0]['scores_3d'].numpy()\n",
    "\n",
    "# Get ground truth boxes and classes of only valid classes\n",
    "gt_annos = dataset.remove_dontcare(input_data['annos'])\n",
    "\n",
    "# Get box mode and points from data\n",
    "box_mode = data['img_metas'][0][0]['box_mode_3d']\n",
    "points   = data['points'][0][0].cpu().detach().numpy()\n",
    "\n",
    "# Get points and bounding boxes in plottable frame\n",
    "points, show_pred_bboxes, show_gt_bboxes = convert_data_2_plot_frame(input_data, points, pred_bboxes, gt_annos)\n",
    "\n",
    "# Generate images showing ground truth, predicted, and lidar points\n",
    "show_results_modified.show_results_modified(\n",
    "    points,\n",
    "    show_gt_bboxes,\n",
    "    show_pred_bboxes,\n",
    "    show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60f028",
   "metadata": {},
   "source": [
    "## Run Model over all LiDAR Scans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ab7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiDAR Scan Number: 100 Elapsed time: 30.688058137893677 Time per LiDAR Scan: 0.30688058853149414\n",
      "LiDAR Scan Number: 200 Elapsed time: 61.755903482437134 Time per LiDAR Scan: 0.30877952218055726\n",
      "LiDAR Scan Number: 300 Elapsed time: 92.91029143333435 Time per LiDAR Scan: 0.30970097382863365\n",
      "LiDAR Scan Number: 400 Elapsed time: 124.19964551925659 Time per LiDAR Scan: 0.3104991161823273\n",
      "LiDAR Scan Number: 500 Elapsed time: 156.1624083518982 Time per LiDAR Scan: 0.312324818611145\n",
      "LiDAR Scan Number: 600 Elapsed time: 187.48369026184082 Time per LiDAR Scan: 0.31247281829516094\n",
      "LiDAR Scan Number: 700 Elapsed time: 219.2533118724823 Time per LiDAR Scan: 0.3132190193448748\n",
      "LiDAR Scan Number: 800 Elapsed time: 250.6405680179596 Time per LiDAR Scan: 0.3133007109165192\n",
      "LiDAR Scan Number: 900 Elapsed time: 283.3333854675293 Time per LiDAR Scan: 0.31481487353642784\n",
      "LiDAR Scan Number: 1000 Elapsed time: 316.4081280231476 Time per LiDAR Scan: 0.3164081287384033\n",
      "LiDAR Scan Number: 1100 Elapsed time: 349.6307649612427 Time per LiDAR Scan: 0.31784615061499855\n",
      "LiDAR Scan Number: 1200 Elapsed time: 383.1702356338501 Time per LiDAR Scan: 0.31930853048960367\n",
      "LiDAR Scan Number: 1300 Elapsed time: 416.3353247642517 Time per LiDAR Scan: 0.320257943043342\n",
      "LiDAR Scan Number: 1400 Elapsed time: 450.05101108551025 Time per LiDAR Scan: 0.32146500842911857\n",
      "LiDAR Scan Number: 1500 Elapsed time: 484.3038115501404 Time per LiDAR Scan: 0.32286920833587646\n",
      "LiDAR Scan Number: 1600 Elapsed time: 517.6791319847107 Time per LiDAR Scan: 0.3235494583845139\n",
      "LiDAR Scan Number: 1700 Elapsed time: 550.8037774562836 Time per LiDAR Scan: 0.3240022225940929\n",
      "LiDAR Scan Number: 1800 Elapsed time: 583.6278829574585 Time per LiDAR Scan: 0.32423771328396267\n",
      "LiDAR Scan Number: 1900 Elapsed time: 616.9387302398682 Time per LiDAR Scan: 0.32470459536502233\n",
      "LiDAR Scan Number: 2000 Elapsed time: 650.7354457378387 Time per LiDAR Scan: 0.3253677232265472\n",
      "LiDAR Scan Number: 2100 Elapsed time: 683.9286606311798 Time per LiDAR Scan: 0.32568031492687405\n",
      "LiDAR Scan Number: 2200 Elapsed time: 717.2028419971466 Time per LiDAR Scan: 0.3260012921420011\n",
      "LiDAR Scan Number: 2300 Elapsed time: 750.2813982963562 Time per LiDAR Scan: 0.32620930412541266\n",
      "LiDAR Scan Number: 2400 Elapsed time: 783.3287949562073 Time per LiDAR Scan: 0.326386998295784\n",
      "LiDAR Scan Number: 2500 Elapsed time: 816.8638439178467 Time per LiDAR Scan: 0.3267455379486084\n",
      "LiDAR Scan Number: 2600 Elapsed time: 851.828911781311 Time per LiDAR Scan: 0.32762650480637184\n",
      "LiDAR Scan Number: 2700 Elapsed time: 885.4233753681183 Time per LiDAR Scan: 0.32793458382288615\n",
      "LiDAR Scan Number: 2800 Elapsed time: 919.0196788311005 Time per LiDAR Scan: 0.32822131420884815\n",
      "LiDAR Scan Number: 2900 Elapsed time: 952.2709214687347 Time per LiDAR Scan: 0.32836928359393414\n",
      "LiDAR Scan Number: 3000 Elapsed time: 986.4399688243866 Time per LiDAR Scan: 0.32881332317988077\n",
      "LiDAR Scan Number: 3100 Elapsed time: 1020.3622150421143 Time per LiDAR Scan: 0.3291491020110346\n",
      "LiDAR Scan Number: 3200 Elapsed time: 1053.048178911209 Time per LiDAR Scan: 0.32907755620777607\n",
      "LiDAR Scan Number: 3300 Elapsed time: 1085.745150089264 Time per LiDAR Scan: 0.3290136820619757\n",
      "LiDAR Scan Number: 3400 Elapsed time: 1118.0583934783936 Time per LiDAR Scan: 0.32884070431484896\n",
      "LiDAR Scan Number: 3500 Elapsed time: 1150.8160014152527 Time per LiDAR Scan: 0.32880457203728813\n",
      "LiDAR Scan Number: 3600 Elapsed time: 1183.1966049671173 Time per LiDAR Scan: 0.32866572380065917\n",
      "LiDAR Scan Number: 3700 Elapsed time: 1215.7557146549225 Time per LiDAR Scan: 0.3285826258401613\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the entire kitti training dataset on model\n",
    "\n",
    "# Set score threshold for prediction visualization\n",
    "score_threshold = 0.5\n",
    "\n",
    "# Initialize results structure\n",
    "results = []\n",
    "count = 0\n",
    "\n",
    "# Start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# Loop through each data set in training set\n",
    "for data in data_loader:\n",
    "\n",
    "    # Get results for each dataset\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, rescale=True, **data)\n",
    "\n",
    "    # Get bounding boxes and scores of model predictions\n",
    "    pred_bboxes = result[0]['boxes_3d'].tensor.numpy()\n",
    "    pred_scores = result[0]['scores_3d'].numpy()\n",
    "\n",
    "    # filter out low score bboxes for visualization\n",
    "    inds = pred_scores > score_threshold\n",
    "    pred_bboxes = pred_bboxes[inds]\n",
    "\n",
    "    # Convert points and predicted bounding boxes to a plotable frame\n",
    "    box_mode = data['img_metas'][0]._data[0][0]['box_mode_3d']\n",
    "    points   = data['points'][0]._data[0][0].cpu().numpy()\n",
    "\n",
    "    # Remove classed in truth data that are \"DontCare\"\n",
    "    gt_annos = dataset.remove_dontcare(dataset.data_infos[count]['annos'])\n",
    "    \n",
    "    # Get points and bounding boxes in plottable frame\n",
    "    points, show_pred_bboxes, show_gt_bboxes = convert_data_2_plot_frame(input_data, points, pred_bboxes, gt_annos)\n",
    "\n",
    "    # Generate images showing ground truth, predicted, and lidar points\n",
    "    show_results_modified.show_results_modified(\n",
    "        points,\n",
    "        show_gt_bboxes,\n",
    "        show_pred_bboxes,\n",
    "        False,\n",
    "        './data/results',\n",
    "        'training_results_file{}'.format(count))\n",
    "\n",
    "    # Place results in list\n",
    "    results.extend(result)\n",
    "    \n",
    "    # Increment counter\n",
    "    count = count + 1\n",
    "    \n",
    "    # Display status\n",
    "    if count%100 == 0:\n",
    "        print('LiDAR Scan Number:', count, 'Elapsed time:', time.time()-t0, 'Time per LiDAR Scan:', (time.time()-t0)/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001fdc9",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60af63aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting prediction to KITTI format\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3769/3769, 605.7 task/s, elapsed: 6s, ETA:     0s\n",
      "Result is saved to /tmp/tmpiritt4ni/results.pkl.\n",
      "\n",
      "\n",
      "----------- AP11 Results ------------\n",
      "\n",
      "Pedestrian AP11@0.50, 0.50, 0.50:\n",
      "bbox AP11:72.4862, 67.5661, 63.9363\n",
      "bev  AP11:68.7299, 62.6823, 56.9828\n",
      "3d   AP11:62.5762, 57.9007, 52.4955\n",
      "aos  AP11:66.32, 60.94, 57.08\n",
      "Pedestrian AP11@0.50, 0.25, 0.25:\n",
      "bbox AP11:72.4862, 67.5661, 63.9363\n",
      "bev  AP11:80.1513, 77.4199, 71.8873\n",
      "3d   AP11:79.8539, 77.3603, 71.7925\n",
      "aos  AP11:66.32, 60.94, 57.08\n",
      "Cyclist AP11@0.50, 0.50, 0.50:\n",
      "bbox AP11:85.6506, 77.0225, 73.8574\n",
      "bev  AP11:82.2988, 67.1939, 63.5440\n",
      "3d   AP11:78.5558, 62.3563, 58.9345\n",
      "aos  AP11:85.45, 75.88, 72.66\n",
      "Cyclist AP11@0.50, 0.25, 0.25:\n",
      "bbox AP11:85.6506, 77.0225, 73.8574\n",
      "bev  AP11:85.8134, 74.9172, 71.5251\n",
      "3d   AP11:85.8134, 74.9172, 71.5251\n",
      "aos  AP11:85.45, 75.88, 72.66\n",
      "Car AP11@0.70, 0.70, 0.70:\n",
      "bbox AP11:90.8448, 89.6040, 88.4684\n",
      "bev  AP11:90.4053, 87.7193, 85.5287\n",
      "3d   AP11:87.2397, 76.9748, 74.4561\n",
      "aos  AP11:90.70, 89.00, 87.46\n",
      "Car AP11@0.70, 0.50, 0.50:\n",
      "bbox AP11:90.8448, 89.6040, 88.4684\n",
      "bev  AP11:90.8402, 90.0159, 89.4945\n",
      "3d   AP11:90.8367, 89.9369, 89.3178\n",
      "aos  AP11:90.70, 89.00, 87.46\n",
      "\n",
      "Overall AP11@easy, moderate, hard:\n",
      "bbox AP11:82.9939, 78.0642, 75.4207\n",
      "bev  AP11:80.4780, 72.5318, 68.6852\n",
      "3d   AP11:76.1239, 65.7440, 61.9620\n",
      "aos  AP11:80.82, 75.27, 72.40\n",
      "\n",
      "----------- AP40 Results ------------\n",
      "\n",
      "Pedestrian AP40@0.50, 0.50, 0.50:\n",
      "bbox AP40:72.9990, 69.0414, 63.5115\n",
      "bev  AP40:68.7132, 62.1466, 56.5481\n",
      "3d   AP40:63.3301, 56.6949, 50.1336\n",
      "aos  AP40:66.27, 61.35, 56.02\n",
      "Pedestrian AP40@0.50, 0.25, 0.25:\n",
      "bbox AP40:72.9990, 69.0414, 63.5115\n",
      "bev  AP40:82.1869, 78.8805, 72.3245\n",
      "3d   AP40:82.0855, 78.7133, 72.2355\n",
      "aos  AP40:66.27, 61.35, 56.02\n",
      "Cyclist AP40@0.50, 0.50, 0.50:\n",
      "bbox AP40:88.5116, 78.3495, 75.3090\n",
      "bev  AP40:83.7727, 67.0453, 63.6856\n",
      "3d   AP40:79.7671, 62.6380, 58.8175\n",
      "aos  AP40:88.26, 77.07, 73.99\n",
      "Cyclist AP40@0.50, 0.25, 0.25:\n",
      "bbox AP40:88.5116, 78.3495, 75.3090\n",
      "bev  AP40:88.5301, 76.0120, 72.6666\n",
      "3d   AP40:88.5301, 76.0120, 72.6666\n",
      "aos  AP40:88.26, 77.07, 73.99\n",
      "Car AP40@0.70, 0.70, 0.70:\n",
      "bbox AP40:96.4778, 92.5885, 89.8701\n",
      "bev  AP40:95.4711, 88.8989, 86.1761\n",
      "3d   AP40:88.6278, 78.4419, 73.9531\n",
      "aos  AP40:96.29, 91.92, 88.82\n",
      "Car AP40@0.70, 0.50, 0.50:\n",
      "bbox AP40:96.4778, 92.5885, 89.8701\n",
      "bev  AP40:96.5541, 95.2315, 92.6746\n",
      "3d   AP40:96.5062, 95.0402, 92.4526\n",
      "aos  AP40:96.29, 91.92, 88.82\n",
      "\n",
      "Overall AP40@easy, moderate, hard:\n",
      "bbox AP40:85.9961, 79.9931, 76.2302\n",
      "bev  AP40:82.6524, 72.6969, 68.8032\n",
      "3d   AP40:77.2417, 65.9249, 60.9680\n",
      "aos  AP40:83.60, 76.78, 72.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method to define pipeline for evaluation for 3D object detection using LiDAR\n",
    "eval_kwargs = cfg.get('evaluation', {}).copy()\n",
    "for key in ['interval', 'tmpdir', 'start', 'gpu_collect', 'save_best','rule']:\n",
    "    eval_kwargs.pop(key, None)\n",
    "    \n",
    "# Setup the logger structure\n",
    "get_root_logger(log_file=None, log_level=0, name='mmdet3d')\n",
    "\n",
    "# Run kitti evaluation\n",
    "scoring_results = dataset.evaluate(results, **eval_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb93e6b",
   "metadata": {},
   "source": [
    "## Create Video from Subset of Training Cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a498eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path and video name\n",
    "imageDirectory = './data/test/'\n",
    "videoFilename = './data/test/training_example_video.avi'\n",
    "\n",
    "# Get all images from training result directory\n",
    "images = [image for image in os.listdir(imageDirectory) if image.endswith(\".png\")]\n",
    "\n",
    "# Read single image to get size of image\n",
    "image = cv2.imread(os.path.join(imageDirectory, images[0]))\n",
    "height, width, _ = image.shape\n",
    "\n",
    "# Start video \n",
    "video = cv2.VideoWriter(videoFilename, 0, 1, (width, height))\n",
    "\n",
    "# Loop through each image and place in video\n",
    "count = 0\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(imageDirectory, image)))\n",
    "    count = count + 1\n",
    "    if count == 100:\n",
    "        break\n",
    "\n",
    "# Close Video\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
